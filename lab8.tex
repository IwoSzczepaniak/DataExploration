\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[polish]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage[T1]{fontenc}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{float}

\title{Eksploracja Danych - labortorium 9}
\author{Iwo Szczepaniak}

\begin{document}
\maketitle

\section{Zbiory danych}
W laboratorium używanych jest 5 starych polskich książek. Są podzielone na różne zbiory danych, w zależności od tego, ile zdań jest w każdym kawałku i na które książki patrzymy:

\begin{itemize}
    \item złożonych z 10, 5, 3 i 1 zdań
    \item obejmujących treść wszystkich książek (five-books*.csv)
    \item obejmujących treść pierwszych dwóch książek (two-books*.csv)
\end{itemize}



\section{AuthorRecognitionDecisionTree}
W tej części laboratorium próbujemy odgadnąć autora za pomocą drzewa decyzyjnego.

\begin{verbatim}
    Distinct authors and their works:
+--------+------------------+
|  author|              work|
+--------+------------------+
| Reymont|   Ziemia obiecana|
|Żuławski|Na srebrnym globie|
+--------+------------------+


Number of documents per author:
+--------+-----+
|  author|count|
+--------+-----+
| Reymont| 1365|
|Żuławski|  394|
+--------+-----+


Average text length per author:
+--------+-----------------+
|  author|  avg_text_length|
+--------+-----------------+
| Reymont|828.3098901098901|
|Żuławski|930.0532994923858|
+--------+-----------------+


Tokenized content sample:
+-------+---------------+--------------------+--------------------+--------------------+
| author|           work|             content|     content_stemmed|               words|
+-------+---------------+--------------------+--------------------+--------------------+
|Reymont|Ziemia obiecana|I Łódź się budził...|i Łódź Łódź się b...|[i, łódź, się, bu...|
|Reymont|Ziemia obiecana|- Zaraz będę budz...|zaraz zaraza być ...|[zaraz, będę, bud...|
|Reymont|Ziemia obiecana|- Ale spaliła się...|Al Ala Ali Alo sp...|[ale, spaliła, si...|
+-------+---------------+--------------------+--------------------+--------------------+
only showing top 3 rows


Bag of Words transformation:

Sample of words and their feature vectors:
+--------------------+--------------------+
|               words|            features|
+--------------------+--------------------+
|[i, łódź, się, bu...|(10000,[0,1,2,4,5...|
|[zaraz, będę, bud...|(10000,[1,2,3,4,8...|
|[ale, spaliła, si...|(10000,[0,1,2,3,4...|
|[moryc, zawołał, ...|(10000,[0,1,2,3,4...|
|[dziwiłem, się, n...|(10000,[0,1,3,5,6...|
+--------------------+--------------------+
only showing top 5 rows


Detailed analysis of first document:
First 20 word frequencies in first document:
i -> 13.000000
się -> 8.000000
w -> 8.000000
na -> 3.000000
z -> 4.000000
do -> 3.000000
a -> 1.000000
co -> 1.000000
za -> 1.000000
jeszcze -> 2.000000
ze -> 1.000000
nad -> 1.000000
nim -> 1.000000
wszystko -> 1.000000
borowiecki -> 1.000000
zaczął -> 1.000000
coraz -> 1.000000
domu -> 1.000000
fabryki -> 1.000000
jakie -> 1.000000

Converting authors to numeric labels:
+--------+-----+
|  author|label|
+--------+-----+
| Reymont|  0.0|
|Żuławski|  1.0|
+--------+-----+


Training Decision Tree classifier:

Making predictions:
+-------+-----+----------+
| author|label|prediction|
+-------+-----+----------+
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
|Reymont|  0.0|       0.0|
+-------+-----+----------+
only showing top 20 rows


Model evaluation:
f1 Score: 0.9972
accuracy Score: 0.9972

Feature Importance Analysis:
Raw feature importances vector:
(10000,[0,1,4,5,7,14,18,42,44,46,68,72,84,93,99,126,185,214,239,268,333,404,478,481,503,551,578,689,726,737,749,818,853,903,911,1071,1076,1088,1214,1269,1448,1512,1528,1738,1789,1796,2461],[0.023887115722551606,0.004632126294739468,0.008312093295560272,0.004433116424298813,0.002955410949532542,0.0027706977651867576,0.020351500744235217,0.006533708088381511,0.003253251570521766,0.0027503249875015638,0.0033007442941790023,0.029523625096953564,0.2204248446714813,0.003277134492271951,0.005171969161681948,0.006368066904000059,0.003220936152029606,0.17397126164134985,0.05414924787635572,0.06952589213215185,0.009220430204750042,0.027261193825600234,0.03431828985126793,0.003282107382244501,0.04720296141707848,0.015325570082783038,0.005171969161681948,0.01593077132154462,0.01540076963790318,0.008471393483807507,0.009498482849887468,0.009540981654584575,0.006480520304874378,0.015438258098096956,0.020660672635427604,0.0032870915999853355,0.012524638502924531,0.006500084139756681,0.004323110984271116,0.02087689333605388,0.009583766325681418,0.015552193582215781,0.003292087179924863,0.003297094156624134,0.009626839432763252,0.006519736699514027,0.012599023883788066])

Most important words for classification:
Top 20 most important words for author classification:
ziemi -> 0.220425
marta -> 0.173971
tom -> 0.069526
piotr -> 0.054149
ada -> 0.047203
wóz -> 0.034318
borowiecki -> 0.029524
morze -> 0.027261
i -> 0.023887
zgoła -> 0.020877
człowieku -> 0.020661
pan -> 0.020352
niebie -> 0.015931
zapewne -> 0.015552
snadź -> 0.015438
gór -> 0.015401
marty -> 0.015326
wybrzeżu -> 0.012599
tomasz -> 0.012525
ziemio -> 0.009627
\end{verbatim}

\section{AuthorRecognitionGridSearchCVDecisionTree}
GridSearchCV to sposób na ulepszenie drzewa decyzyjnego przez wypróbowanie różnych ustawień, np. jak duży ma być słownik.

\begin{verbatim}
    Best model parameters:
RegexTokenizer: uid=regexTok_6ef4f1296085, minTokenLength=1, gaps=true, pattern=[\s\p{Punct}\u2014\u2026\u201C\u201E]+, toLowercase=true
CountVectorizerModel: uid=cntVec_1172c411247f, vocabularySize=10000
StringIndexerModel: uid=strIdx_a85a31ed031a, handleInvalid=error
DecisionTreeClassificationModel: uid=dtc_14dfe855aeaa, depth=30, numNodes=103, numClasses=2, numFeatures=10000

Average metrics for all parameter combinations:
Parameter set 1: F1 = 0.8465
Parameter set 2: F1 = 0.9236
Parameter set 3: F1 = 0.9218
Parameter set 4: F1 = 0.8431
Parameter set 5: F1 = 0.9257
Parameter set 6: F1 = 0.9298
Parameter set 7: F1 = 0.8418
Parameter set 8: F1 = 0.9237
Parameter set 9: F1 = 0.9331

Test set evaluation metrics:
accuracy: 0.9304
weightedPrecision: 0.9340
weightedRecall: 0.9304
f1: 0.9317
\end{verbatim}


\section{AuthorRecognitionCVDecisionTree}
Walidacja krzyżowa dla drzewa decyzyjnego. Sprawdzamy jak dobrze działa na różnych zbiorach danych, w postaci tabelki:

\begin{verbatim}
    Evaluation Results:
------------------------------------------------------------------------------------------------------------
File                                          | Accuracy   | Precision  | Recall     | F1        
------------------------------------------------------------------------------------------------------------
two-books-all-1000-1-stem.csv                 | 0.8277     | 0.8182     | 0.8277     | 0.8007    
two-books-all-1000-3-stem.csv                 | 0.8784     | 0.8743     | 0.8784     | 0.8716    
two-books-all-1000-5-stem.csv                 | 0.9214     | 0.9200     | 0.9214     | 0.9183    
two-books-all-1000-10-stem.csv                | 0.9150     | 0.9126     | 0.9150     | 0.9129    
five-books-all-1000-1-stem.csv                | 0.5678     | 0.6518     | 0.5678     | 0.4746    
five-books-all-1000-3-stem.csv                | 0.6678     | 0.7066     | 0.6678     | 0.6559    
five-books-all-1000-5-stem.csv                | 0.7541     | 0.7680     | 0.7541     | 0.7476    
five-books-all-1000-10-stem.csv               | 0.8567     | 0.8622     | 0.8567     | 0.8539    
------------------------------------------------------------------------------------------------------------
\end{verbatim}


\section{NaiveBayesDemo}
Test Naiwny Bayesa na prostym przykładzie, ze szczególnym uwzględnieniem czemu ustawienie 'smoothing' na zero to zły pomysł.
\begin{verbatim}
    After tokenization:
+------+---------------+--------------------+
|author|        content|               words|
+------+---------------+--------------------+
|   Ala|aaa aaa bbb ccc|[aaa, aaa, bbb, ccc]|
|   Ala|    aaa bbb ddd|     [aaa, bbb, ddd]|
|   Ala|        aaa bbb|          [aaa, bbb]|
|   Ala|    aaa bbb bbb|     [aaa, bbb, bbb]|
|   Ola|    aaa ccc ddd|     [aaa, ccc, ddd]|
|   Ola|    bbb ccc ddd|     [bbb, ccc, ddd]|
|   Ola|    ccc ddd eee|     [ccc, ddd, eee]|
+------+---------------+--------------------+

-----------
After feature extraction and label indexing:
+------+---------------+--------------------+--------------------+-----+
|author|        content|               words|            features|label|
+------+---------------+--------------------+--------------------+-----+
|   Ala|aaa aaa bbb ccc|[aaa, aaa, bbb, ccc]|(5,[0,1,2],[2.0,1...|  0.0|
|   Ala|    aaa bbb ddd|     [aaa, bbb, ddd]|(5,[0,1,3],[1.0,1...|  0.0|
|   Ala|        aaa bbb|          [aaa, bbb]| (5,[0,1],[1.0,1.0])|  0.0|
|   Ala|    aaa bbb bbb|     [aaa, bbb, bbb]| (5,[0,1],[1.0,2.0])|  0.0|
|   Ola|    aaa ccc ddd|     [aaa, ccc, ddd]|(5,[0,2,3],[1.0,1...|  1.0|
|   Ola|    bbb ccc ddd|     [bbb, ccc, ddd]|(5,[1,2,3],[1.0,1...|  1.0|
|   Ola|    ccc ddd eee|     [ccc, ddd, eee]|(5,[2,3,4],[1.0,1...|  1.0|
+------+---------------+--------------------+--------------------+-----+

Naive Bayes parameters explanation:
featuresCol: features column name (default: features, current: features)
labelCol: label column name (default: label, current: label)
modelType: The model type which is a string (case-sensitive). Supported options: multinomial (default), complement, bernoulli and gaussian. (default: multinomial, current: multinomial)
predictionCol: prediction column name (default: prediction)
probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)
rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)
smoothing: The smoothing parameter. (default: 1.0, current: 0.01)
thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)
weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)

Model Parameters Analysis
Vocabulary and Labels:

Vocabulary:
0: aaa
1: bbb
2: ccc
3: ddd
4: eee

Labels:
0: Ala
1: Ola

Conditional Probabilities (Likelihood):
P(aaa|Ala)=0.415768 (log=-0.877629)
P(bbb|Ala)=0.415768 (log=-0.877629)
P(ccc|Ala)=0.083817 (log=-2.479114)
P(ddd|Ala)=0.083817 (log=-2.479114)
P(eee|Ala)=0.000830 (log=-7.094235)
P(aaa|Ola)=0.111602 (log=-2.192814)
P(bbb|Ola)=0.111602 (log=-2.192814)
P(ccc|Ola)=0.332597 (log=-1.100825)
P(ddd|Ola)=0.332597 (log=-1.100825)
P(eee|Ola)=0.111602 (log=-2.192814)

Prior Probabilities:
P(Ala)=0.571225 (log=-0.559972)
P(Ola)=0.428775 (log=-0.846823)

Prediction Test with smoothing=0.01
Manual calculation:
log(p0)=-8.87494 p0=0.000139850
log(p1)=-6.34211 p1=0.00176058
Manual classification result: 1

Model raw probabilities:
Pr:[0.000140, 0.001761]
Model predicted Label: 1 (Ola)

Testing with smoothing = 0:

Conditional Probabilities with no smoothing:
P(aaa|Ala)=0.416667 (log=-0.875469)
P(bbb|Ala)=0.416667 (log=-0.875469)
P(ccc|Ala)=0.083333 (log=-2.484907)
P(ddd|Ala)=0.083333 (log=-2.484907)
P(eee|Ala)=0.000000 (log=-Infinity)
P(aaa|Ola)=0.111111 (log=-2.197225)
P(bbb|Ola)=0.111111 (log=-2.197225)
P(ccc|Ola)=0.333333 (log=-1.098612)
P(ddd|Ola)=0.333333 (log=-1.098612)
P(eee|Ola)=0.111111 (log=-2.197225)

Raw probabilities with no smoothing:
Pr:[NaN, 0.001764]
\end{verbatim}

\section{AuthorRecognitionGridSearchCVNaiveBayes}
Podobne podejście do GridSearchCV, ale dla Naiwnego Bayesa. Szukane są różne parametry, jak typ modelu i rozmiar słownika, szukając optymalnych.

\begin{verbatim}
    Best model parameters:
RegexTokenizer: uid=regexTok_3ed76e4949bb, minTokenLength=1, gaps=true, pattern=[\s\p{Punct}\u2014\u2026\u201C\u201E]+, toLowercase=true
CountVectorizerModel: uid=cntVec_319ad0ebafda, vocabularySize=10000
StringIndexerModel: uid=strIdx_a63926c35035, handleInvalid=error
NaiveBayesModel: uid=nb_edd3a224d41a, modelType=multinomial, numClasses=2, numFeatures=10000

Average metrics for all parameter combinations:
Parameter set 1: F1 = 0.7711
Parameter set 2: F1 = 0.8739
Parameter set 3: F1 = 0.9076
Parameter set 4: F1 = 0.9135
Parameter set 5: F1 = 0.4147
Parameter set 6: F1 = 0.6037
Parameter set 7: F1 = 0.7477
Parameter set 8: F1 = 0.7838

Test set evaluation metrics:
accuracy: 0.9133
weightedPrecision: 0.9111
weightedRecall: 0.9133
f1: 0.9107
\end{verbatim}

\section{AuthorRecognitionCVNaiveBayes}
Walidacja krzyżowa dla Naiwnego Bayesa, dla różnych zbiorów danych w postaci tabelki:

\begin{verbatim}
| Dataset                             |    CV F1 | Accuracy | Precision |   Recall |       F1 |
|-------------------------------------|----------|----------|-----------|----------|----------|
| two-books-all-1000-1-stem.csv       |   0.9082 |   0.9093 |    0.9072 |   0.9093 |   0.9065 |
| two-books-all-1000-3-stem.csv       |   0.9764 |   0.9755 |    0.9755 |   0.9755 |   0.9755 |
| two-books-all-1000-5-stem.csv       |   0.9897 |   0.9828 |    0.9829 |   0.9828 |   0.9827 |
| two-books-all-1000-10-stem.csv      |   0.9972 |   0.9938 |    0.9938 |   0.9938 |   0.9938 |
| five-books-all-1000-1-stem.csv      |   0.7509 |   0.7671 |    0.7642 |   0.7671 |   0.7614 |
| five-books-all-1000-3-stem.csv      |   0.9114 |   0.9162 |    0.9158 |   0.9162 |   0.9157 |
| five-books-all-1000-5-stem.csv      |   0.9587 |   0.9552 |    0.9551 |   0.9552 |   0.9548 |
| five-books-all-1000-10-stem.csv     |   0.9897 |   0.9915 |    0.9915 |   0.9915 |   0.9915 |
\end{verbatim}

\end{document}